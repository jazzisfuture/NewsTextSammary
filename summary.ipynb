{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd05509c4c259a5a49102f826d234aea18f0c83b903638cb80c1a627f502e3896ba",
   "display_name": "Python 3.7.7 64-bit ('pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn \n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "import bert_seq2seq\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from bert_seq2seq.tokenizer import Tokenizer, load_chinese_base_vocab\n",
    "from bert_seq2seq.utils import load_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "精简后的词表大小为：13584\n"
     ]
    }
   ],
   "source": [
    "vocab_path = \"./vocab/bert_wmm_chinese_vocab.txt\"\n",
    "word2idx, keep_tokens = load_chinese_base_vocab(vocab_path=vocab_path, simplfied=True)\n",
    "model_name = 'bert'\n",
    "model_path = \"./model_file/torch_model.bin\"\n",
    "model_save_path = \"./model_file/trained_model/summary.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/train_with_summ.csv\")\n",
    "del df[\"Unnamed: 0\"]\n",
    "train_len = df.article.__len__() // 10 * 8 \n",
    "train_dataset = df[:train_len]\n",
    "eval_dataset = df[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "【环球网报道记者王欢】日本共同社8月18日从日内阁府的分析中获悉，对1972年至2013年42年期间18岁以下日本未成年人自杀日期按日统计后发现，最多的是暑假结束后的9月1日，为131人。数据显示长假结束后的自杀倾向明显，日本文部科学省呼吁“在老师照顾不到的假期中，希望家庭关注孩子的行动、着装变化和身体状况等”。据共同社8月19日报道，日本内阁府基于厚生劳动省的《人口动态调查》信息，将总计18048名自杀的18岁以下人群按日进行了分析。继9月1日之后，较多的依次为4月11日(99人)、4月8日(95人)、9月2日(94人)、8月31日(92人)，据分析新学期开始前后有增加的倾向。暑假7月下旬至8月中旬自杀者较少。帮助拒绝上学的孩子及其父母的法人NPO也于18日发出紧急呼吁：“若感到上学痛苦就先休息一段时间。”在日本内阁府等汇总的信息中，小学及初中生自杀原因中“来自家人的管教和斥责”、“亲子关系不好”等家庭生活因素居多，而高中生中因“成绩不好”、“为前途烦恼”等比例增加，抑郁症等精神疾病也成为主要原因。因日本青少年人群自杀率减少幅度小于中老年人群，内阁府分析了厚生劳动省的调查结果。与其他年龄层相比，10至15岁儿童有无预兆自杀的倾向，内阁府指出“大人创造使孩子易于向周围倾诉烦恼的环境十分重要”。（完）\n中新网兰州2月22日电(记者冯志军)据中国地震台网测定，2月22日12时56分15秒，甘肃武威市天祝藏族自治县发生4.3级地震，震中位于北纬37.6°，东经102.3°，震源深度6千米。记者震后联系天祝县委宣传部工作人员了解到，天祝县城震感并不明显，目前户外民众情绪平稳，工作和休闲并未受此次地震影响，一切正常。不过，武威市区震感明显。截至13时发稿前，目前仍未有此次地震造成的灾情出现。(完)（完）\n"
     ]
    }
   ],
   "source": [
    "for text in eval_dataset[:2][\"article\"]:\n",
    "    print(text+\"（完）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "lr = 1e-5\n",
    "maxlen=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    \"\"\"\n",
    "    针对特定数据集，定义相关的取数据方式\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BertDataset, self).__init__()\n",
    "        # 拿到数据集\n",
    "        \n",
    "        self.dataset = train_dataset\n",
    "        # 词->id\n",
    "        self.idx2word = {k: v for v, k in word2idx.items()}\n",
    "        # 分词器\n",
    "        self.tokenizer = Tokenizer(word2idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 得到单个数据\n",
    "        summary, article = self.dataset[\"summary\"][i], self.dataset[\"article\"][i]\n",
    "        # print(article)\n",
    "        # print(summary)\n",
    "        token_ids, token_type_ids = self.tokenizer.encode(\n",
    "            article, summary, max_length=maxlen\n",
    "        )\n",
    "        output = {\n",
    "            \"token_ids\": token_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "        }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    动态padding，batch为一部分sample\n",
    "    \"\"\"\n",
    "    def padding(indice, max_length, pad_idx=0):\n",
    "        \"\"\"\n",
    "        pad函数\n",
    "        \"\"\"\n",
    "        pad_indice = [item + [pad_idx] * max(0, max_length-len(item)) for item in indice]\n",
    "        \n",
    "        return torch.tensor(pad_indice)\n",
    "    token_ids = [data[\"token_ids\"]for data in batch]\n",
    "    max_length = max([len(t) for t in token_ids])\n",
    "    token_type_ids = (data[\"token_type_ids\"] for data in batch)\n",
    "\n",
    "    token_ids_padded = padding(token_ids, max_length)\n",
    "    token_type_ids_padded = padding(token_type_ids, max_length)\n",
    "    # 任务目标\n",
    "    target_ids_padded = token_ids_padded[:,1:].contiguous()\n",
    "\n",
    "    return token_ids_padded, token_type_ids_padded, target_ids_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        # 判断是否有可用GPU\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"device: \" + str(self.device))\n",
    "        # 定义模型\n",
    "        self.bert_model = load_bert(word2idx, model_name=model_name)\n",
    "        ## 加载预训练的模型参数～\n",
    "        \n",
    "        self.bert_model.load_pretrain_params(model_path, keep_tokens=keep_tokens)\n",
    "        # 加载已经训练好的模型，继续训练\n",
    "\n",
    "        # 将模型发送到计算设备(GPU或CPU)\n",
    "        self.bert_model.set_device(self.device)\n",
    "        # 声明需要优化的参数\n",
    "        self.optim_parameters = list(self.bert_model.parameters())\n",
    "        self.optimizer = torch.optim.Adam(self.optim_parameters, lr=lr, weight_decay=1e-3)\n",
    "        # 声明自定义的数据加载器\n",
    "        dataset = BertDataset()\n",
    "        self.dataloader =  DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    def train(self, epoch):\n",
    "        # 一个epoch的训练\n",
    "        self.bert_model.train()\n",
    "        self.iteration(epoch, dataloader=self.dataloader, train=True)\n",
    "    \n",
    "    def save(self, save_path):\n",
    "        \"\"\"\n",
    "        保存模型\n",
    "        \"\"\"\n",
    "        self.bert_model.save_all_params(save_path)\n",
    "        print(\"{} saved!\".format(save_path))\n",
    "\n",
    "    def iteration(self, epoch, dataloader, train=True):\n",
    "        total_loss = 0\n",
    "        start_time = time.time() ## 得到当前时间\n",
    "        step = 0\n",
    "        report_loss = 0\n",
    "        for token_ids, token_type_ids, target_ids in tqdm(dataloader,position=0, leave=True):\n",
    "            step += 1\n",
    "            if step % 1000 == 0:\n",
    "                self.bert_model.eval()\n",
    "                test_data = eval_dataset[:200]\n",
    "                for text in test_data:\n",
    "                    print(self.bert_model.generate(text, beam_size=3))\n",
    "                print(\"loss is \" + str(report_loss))\n",
    "                report_loss = 0\n",
    "                # self.eval(epoch)\n",
    "                self.bert_model.train()\n",
    "            if step % 8000 == 0:\n",
    "                self.save(model_save_path)\n",
    "\n",
    "            # 因为传入了target标签，因此会计算loss并且返回\n",
    "            predictions, loss = self.bert_model(token_ids,\n",
    "                                                token_type_ids,\n",
    "                                                labels=target_ids,\n",
    "                                               \n",
    "                                                )\n",
    "            report_loss += loss.item()\n",
    "            # 反向传播\n",
    "            if train:\n",
    "                # 清空之前的梯度\n",
    "                self.optimizer.zero_grad()\n",
    "                # 反向传播, 获取新的梯度\n",
    "                loss.backward()\n",
    "                # 用获取的梯度更新模型参数\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # 为计算当前epoch的平均loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        end_time = time.time()\n",
    "        spend_time = end_time - start_time\n",
    "        # 打印训练信息\n",
    "        print(\"epoch is \" + str(epoch)+\". loss is \" + str(total_loss) + \". spend time is \"+ str(spend_time))\n",
    "        # 保存模型\n",
    "        self.save(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device: cuda\n",
      "./model_file/torch_model.bin loaded!\n",
      "  0%|          | 0/6667 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 6.00 GiB total capacity; 4.43 GiB already allocated; 44.45 MiB free; 4.55 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d55dbcaf326c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_epoches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# 训练一个epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-43babf47ff7c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# 一个epoch的训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-43babf47ff7c>\u001b[0m in \u001b[0;36miteration\u001b[1;34m(self, epoch, dataloader, train)\u001b[0m\n\u001b[0;32m     54\u001b[0m             predictions, loss = self.bert_model(token_ids,\n\u001b[0;32m     55\u001b[0m                                                 \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                                                 \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                                                 )\n",
      "\u001b[1;32mD:\\python\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\我的坚果云\\毕业论文\\code\\bert_seq2seq-master\\bert_seq2seq-master\\bert_seq2seq\\seq2seq_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_tensor, token_type_id, position_enc, labels)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         enc_layers, _ = self.bert(input_tensor, position_ids=position_enc, token_type_ids=token_type_id, attention_mask=a_mask, \n\u001b[1;32m--> 100\u001b[1;33m                                     output_all_encoded_layers=True)\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[0msquence_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m## 取出来最后一层输出\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\我的坚果云\\毕业论文\\code\\bert_seq2seq-master\\bert_seq2seq-master\\bert_seq2seq\\model\\bert_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, output_all_encoded_layers, output_attentions)\u001b[0m\n\u001b[0;32m    462\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_encoded_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m         )\n\u001b[0;32m    466\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\我的坚果云\\毕业论文\\code\\bert_seq2seq-master\\bert_seq2seq-master\\bert_seq2seq\\model\\bert_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_all_encoded_layers, output_attentions)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             layer_output, attention_matrix = layer_module(\n\u001b[1;32m--> 284\u001b[1;33m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m             )\n\u001b[0;32m    286\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\我的坚果云\\毕业论文\\code\\bert_seq2seq-master\\bert_seq2seq-master\\bert_seq2seq\\model\\bert_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     ):\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mattention_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\我的坚果云\\毕业论文\\code\\bert_seq2seq-master\\bert_seq2seq-master\\bert_seq2seq\\model\\bert_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mself_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_metrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\我的坚果云\\毕业论文\\code\\bert_seq2seq-master\\bert_seq2seq-master\\bert_seq2seq\\model\\bert_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# 注意力加权 torch.dot()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m    981\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[0;32m    982\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[0;32m    984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 6.00 GiB total capacity; 4.43 GiB already allocated; 44.45 MiB free; 4.55 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "train_epoches = 10\n",
    "\n",
    "for epoch in range(train_epoches):\n",
    "    # 训练一个epoch\n",
    "    trainer.train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3275"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "pd.read_csv(\"./dataset/train_with_summ.csv\")['article'].max().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "精简后的词表大小为：13584\n"
     ]
    }
   ],
   "source": [
    "test_word2idx, test_keep_tokens = load_chinese_base_vocab(vocab_path=vocab_path, simplfied=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(dict, list)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "type(test_word2idx), type(test_keep_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "中新社西宁11月22日电(赵凛松)青海省林业厅野生动植物和自然保护区管理局高级工程师张毓22日向中新社记者确认：“经过中国林业科学院、中科院新疆生态与地理研究所和青海省林业厅的共同认定，出现在青海省海西州境内的三只体型较大的鸟为世界极度濒危的红鹳目红鹳科红鹳属的大红鹳。”11月18日，青海省海西州可鲁克湖—托素湖国家级陆生野生动物疫源疫病监测站在野外监测巡护过程中，在可鲁克湖西南岸入水口盐沼滩发现三只体型较大的鸟类。张毓说：“此前在该区域从未发现过这种体型的鸟类。”可鲁克湖—托素湖位于青海省柴达木盆地东北部，海拔2800米，水域湿地环境内的优势种动物主要是水禽，共有30余种。根据拍摄的照片以及视频，张毓根据动物学体型得出了初步结论，然后会同中国林业科学院和中科院新疆生态与地理研究所的相关专家，确认了这三只鸟为红鹳目红鹳科红鹳属的大红鹳。大红鹳也称为大火烈鸟、红鹤等，三只鸟类特征为大红鹳亚成体。根据世界自然保护联盟、世界濒危动物红色名录，该鸟主要分布于非洲、中亚、南亚等区域，分布广、种群数量较大，无威胁因子，以往在中国并无分布。但1997年在新疆野外首次发现并确定该鸟在中国境内有分布，为中国鸟类新纪录，2012年在四川也发现一只该鸟亚成体。此次野外发现在中国属第三次。“我们现在还无法判断这三只鸟从何而来。不过我个人倾向于是从中亚国家迁徙至此。”张毓强调说，该种鸟国内也有人工饲养，因此也有人判断为从动物园逃逸。“我们对这三只鸟进行了详尽的记录，如果明年这个时间还在此地出现这种鸟，那就能肯定是迁徙的鸟类，而不是从动物园里跑出来的。”由于目前可鲁克湖—托素湖已开始结冰，鸟类采食困难，不排除三只鸟由于无法获得能量补给而进行远距离迁飞的可能。青海省林业厅野生动物行政主管部门将随时做好野外救护的各项准备工作。(完)\n青海首次野外发现濒危大火烈鸟 尚不清楚具体来源\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'token_ids': [2,\n",
       "  602,\n",
       "  3071,\n",
       "  4750,\n",
       "  6103,\n",
       "  2021,\n",
       "  8008,\n",
       "  3197,\n",
       "  8027,\n",
       "  3087,\n",
       "  4408,\n",
       "  11,\n",
       "  6525,\n",
       "  1021,\n",
       "  3249,\n",
       "  12,\n",
       "  7369,\n",
       "  3760,\n",
       "  4587,\n",
       "  3258,\n",
       "  587,\n",
       "  1222,\n",
       "  6927,\n",
       "  4393,\n",
       "  1118,\n",
       "  3388,\n",
       "  4187,\n",
       "  1367,\n",
       "  5530,\n",
       "  4095,\n",
       "  822,\n",
       "  2742,\n",
       "  1175,\n",
       "  4950,\n",
       "  4313,\n",
       "  2127,\n",
       "  7668,\n",
       "  5175,\n",
       "  2237,\n",
       "  4821,\n",
       "  2258,\n",
       "  2374,\n",
       "  3580,\n",
       "  8027,\n",
       "  3087,\n",
       "  1301,\n",
       "  602,\n",
       "  3071,\n",
       "  4750,\n",
       "  6279,\n",
       "  5340,\n",
       "  4700,\n",
       "  6269,\n",
       "  7936,\n",
       "  1,\n",
       "  5205,\n",
       "  6712,\n",
       "  602,\n",
       "  1642,\n",
       "  3258,\n",
       "  587,\n",
       "  4804,\n",
       "  2008,\n",
       "  7266,\n",
       "  408,\n",
       "  602,\n",
       "  4804,\n",
       "  7266,\n",
       "  3071,\n",
       "  4436,\n",
       "  4393,\n",
       "  2476,\n",
       "  578,\n",
       "  1663,\n",
       "  4313,\n",
       "  4675,\n",
       "  4853,\n",
       "  2690,\n",
       "  1367,\n",
       "  7369,\n",
       "  3760,\n",
       "  4587,\n",
       "  3258,\n",
       "  587,\n",
       "  1222,\n",
       "  4536,\n",
       "  964,\n",
       "  1296,\n",
       "  6269,\n",
       "  2035,\n",
       "  7922,\n",
       "  1037,\n",
       "  4283,\n",
       "  1660,\n",
       "  7369,\n",
       "  3760,\n",
       "  4587,\n",
       "  3760,\n",
       "  6103,\n",
       "  2234,\n",
       "  1760,\n",
       "  977,\n",
       "  4536,\n",
       "  574,\n",
       "  1270,\n",
       "  758,\n",
       "  1696,\n",
       "  6670,\n",
       "  1818,\n",
       "  4536,\n",
       "  7779,\n",
       "  609,\n",
       "  584,\n",
       "  4416,\n",
       "  3251,\n",
       "  2326,\n",
       "  3983,\n",
       "  1212,\n",
       "  4536,\n",
       "  5171,\n",
       "  7815,\n",
       "  4578,\n",
       "  5171,\n",
       "  7815,\n",
       "  4804,\n",
       "  5171,\n",
       "  7815,\n",
       "  2145,\n",
       "  4536,\n",
       "  1818,\n",
       "  5171,\n",
       "  7815,\n",
       "  409,\n",
       "  1,\n",
       "  8008,\n",
       "  3197,\n",
       "  8020,\n",
       "  3087,\n",
       "  7922,\n",
       "  7369,\n",
       "  3760,\n",
       "  4587,\n",
       "  3760,\n",
       "  6103,\n",
       "  2234,\n",
       "  1275,\n",
       "  7724,\n",
       "  944,\n",
       "  3857,\n",
       "  1,\n",
       "  2703,\n",
       "  5060,\n",
       "  3857,\n",
       "  1642,\n",
       "  2055,\n",
       "  5175,\n",
       "  7253,\n",
       "  4393,\n",
       "  6927,\n",
       "  4393,\n",
       "  1118,\n",
       "  4187,\n",
       "  4452,\n",
       "  3873,\n",
       "  4452,\n",
       "  4465,\n",
       "  4562,\n",
       "  3742,\n",
       "  4889,\n",
       "  1660,\n",
       "  6927,\n",
       "  1810,\n",
       "  4562,\n",
       "  3742,\n",
       "  2235,\n",
       "  2742,\n",
       "  6712,\n",
       "  4821,\n",
       "  602,\n",
       "  7922,\n",
       "  1660,\n",
       "  1275,\n",
       "  7724,\n",
       "  944,\n",
       "  3857,\n",
       "  6103,\n",
       "  1196,\n",
       "  2177,\n",
       "  955,\n",
       "  3615,\n",
       "  1264,\n",
       "  4561,\n",
       "  3679,\n",
       "  3911,\n",
       "  1253,\n",
       "  4283,\n",
       "  574,\n",
       "  1270,\n",
       "  758,\n",
       "  1696,\n",
       "  6670,\n",
       "  1818,\n",
       "  4536,\n",
       "  7779,\n",
       "  5000,\n",
       "  409,\n",
       "  2374,\n",
       "  3580,\n",
       "  6330,\n",
       "  7936,\n",
       "  1,\n",
       "  3532,\n",
       "  1082,\n",
       "  1660,\n",
       "  6319,\n",
       "  1175,\n",
       "  1716,\n",
       "  692,\n",
       "  3211,\n",
       "  1253,\n",
       "  4283,\n",
       "  6712,\n",
       "  6719,\n",
       "  4803,\n",
       "  758,\n",
       "  1696,\n",
       "  4536,\n",
       "  7779,\n",
       "  5000,\n",
       "  409,\n",
       "  1,\n",
       "  1275,\n",
       "  3,\n",
       "  7369,\n",
       "  3760,\n",
       "  7572,\n",
       "  3511,\n",
       "  6927,\n",
       "  1810,\n",
       "  1253,\n",
       "  4283,\n",
       "  3983,\n",
       "  1212,\n",
       "  1818,\n",
       "  4023,\n",
       "  4062,\n",
       "  7779,\n",
       "  2111,\n",
       "  577,\n",
       "  3824,\n",
       "  3402,\n",
       "  970,\n",
       "  758,\n",
       "  3239,\n",
       "  3873,\n",
       "  3],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "test_dataset = BertDataset()\n",
    "test_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "[1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  1].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}